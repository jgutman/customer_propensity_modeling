{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "from subscriber_learn.utils.s3_read_write import S3ReadWrite\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import linear_model, feature_selection, preprocessing\n",
    "from subscriber_learn.cross_validation.pipeline_tools import DummyEncoder, Timer\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_folds = 5;\n",
    "offset = 7;\n",
    "input_dir = 'input_files/etlv_modified'\n",
    "response_dir = 'input_files/responses/canceled_within_7_days'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_end_date = datetime.date(2018, 1, 28)\n",
    "context = {'ds': train_end_date.strftime('%Y-%m-%d'),\n",
    "               'execution_date': train_end_date}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(s3_reader, n_folds, offset, input_dir, response_dir, **context):\n",
    "    logging.info('Reading data from {}.\\nInput: {}\\nOutput: {}'.format(\n",
    "        str(s3_reader), input_dir, response_dir))\n",
    "    date_fmt = '%Y-%m-%d'\n",
    "    filename = 'part000.csv000'\n",
    "    dates = [context['execution_date'] - datetime.timedelta(days = offset * i)\n",
    "        for i in range(n_folds)]\n",
    "\n",
    "    input_data = [s3_reader.read_from_S3_csv(\n",
    "            csv_name = '{dir}/{year}/{month}/{day}/{filename}'.format(\n",
    "                dir = input_dir,\n",
    "                year = input_date.year,\n",
    "                month = input_date.month,\n",
    "                day = input_date.day,\n",
    "                filename = filename))\n",
    "        .assign(input_date = input_date)\n",
    "        .set_index(['internal_user_id', 'input_date'])\n",
    "        for input_date in dates]\n",
    "    logging.info('{} input files read'.format(len(input_data)))\n",
    "\n",
    "    response_data = [s3_reader.read_from_S3_csv(\n",
    "            csv_name = '{dir}/{year}/{month}/{day}/{filename}'.format(\n",
    "                dir = response_dir,\n",
    "                year = date.year,\n",
    "                month = date.month,\n",
    "                day = date.day,\n",
    "                filename = filename))\n",
    "        .assign(input_date = date)\n",
    "        .set_index(['internal_user_id', 'input_date'])\n",
    "        for date in dates]\n",
    "    logging.info('{} response files read'.format(len(response_data)))\n",
    "\n",
    "    check_cols = ['valid_account_creation',\n",
    "        'valid_prospect_creation',\n",
    "        'valid_accepted_terms']\n",
    "\n",
    "    input_data = pd.concat(input_data)\n",
    "\n",
    "    input_data = (input_data.loc[\n",
    "        input_data.valid_account_creation & \\\n",
    "        input_data.valid_prospect_creation & \\\n",
    "        input_data.valid_accepted_terms, :]\n",
    "        .drop(check_cols, axis = 1))\n",
    "    logging.info('Input shape: {}'.format(input_data.shape))\n",
    "\n",
    "    response_data = (pd.concat(response_data)\n",
    "        .loc[input_data.index])\n",
    "    logging.info('Response shape: {}'.format(response_data.shape))\n",
    "\n",
    "    return input_data, response_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data, response_data = read_data(\n",
    "        S3ReadWrite(), n_folds, offset, input_dir, response_dir,\n",
    "        **context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_temporal_cv(X, fold_col = 'input_date'):\n",
    "    time_index = X.index.get_level_values(fold_col)\n",
    "    cv = TemporalCrossValidator(time_index)\n",
    "    return cv.split()\n",
    "\n",
    "class TemporalCrossValidator():\n",
    "    def __init__(self, time_index, n_weeks = 1):\n",
    "        self.n_weeks = n_weeks\n",
    "        self.unique_groups, self.groups = np.unique(\n",
    "            time_index, return_inverse=True)\n",
    "        n_groups = len(self.unique_groups)\n",
    "        self.n_splits = n_groups - n_weeks\n",
    "        if self.n_splits <= 1:\n",
    "            raise ValueError(\n",
    "                \"Found {} time periods in the data, for training folds \"\n",
    "                \"consisting of {} time periods of input per fold. \"\n",
    "                \"Cross-validation requires 2 or more folds\".format(\n",
    "                n_groups, n_weeks))\n",
    "\n",
    "    def split(self):\n",
    "        self.groups = check_array(groups, ensure_2d=False, dtype=None)\n",
    "\n",
    "        for fold_index in range(self.n_splits):\n",
    "            train_indices = range(fold_index, fold_index + n_weeks)\n",
    "            test_index = fold_index + n_weeks\n",
    "            yield np.where(np.isin(groups, train_indices))[0], \\\n",
    "                np.where(groups == test_index)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = linear_model.ElasticNetCV(\n",
    "            l1_ratio = [.1, .5, .7, .9, .95, 1],\n",
    "            cv = get_temporal_cv(input_data),\n",
    "            n_jobs = -1,\n",
    "            verbose = 1,\n",
    "            random_state = 1100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = make_pipeline(DummyEncoder(),\n",
    "            preprocessing.Imputer(strategy = 'median'),\n",
    "            preprocessing.RobustScaler(),\n",
    "            feature_selection.VarianceThreshold(threshold = .04))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('dummyencoder', DummyEncoder()), ('imputer', Imputer(axis=0, copy=True, missing_values='NaN', strategy='median', verbose=0)), ('robustscaler', RobustScaler(copy=True, quantile_range=(25.0, 75.0), with_centering=True,\n",
       "       with_scaling=True)), ('variancethreshold', VarianceThreshold(threshold=0.04))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(input_data, response_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(358514, 168)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.transform(input_data).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(272,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.named_steps['variancethreshold'].get_support().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(272,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.transformed_columns.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.named_steps['variancethreshold'].get_support().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
